from fastapi import APIRouter, Depends, HTTPException, status, File, UploadFile, Form, Query
from sqlalchemy.orm import Session
from typing import Optional, List, Dict, Any
from uuid import UUID
from sqlalchemy import text
import os
import glob
import aiofiles
from datetime import datetime
import hashlib
import json

from ...database import get_db
from ...config import settings
from ...models.document import COADocument, ProcessingStatus
from ...models.extracted_data import ExtractedData
from ...schemas.document import (
    DirectoryProcessRequest
)
from ...schemas.extracted_data import ProcessingResultResponse, ApiResponse
from ...services.file_manager import FileManager
from ...services.pdf_processor import PDFProcessor
from ...services.ai_extractor import AIExtractor

router = APIRouter()

# Initialize services
file_manager = FileManager(settings.UPLOAD_DIR)
pdf_processor = PDFProcessor()
ai_extractor = AIExtractor()

# ============ ç¼“å­˜ç›¸å…³å‡½æ•° ============

class BatchDataCache:
    """æ‰¹æ¬¡æ•°æ®ç¼“å­˜ç®¡ç†å™¨"""
    def __init__(self, db: Session):
        self.db = db
        self._create_table_if_not_exists()
    
    def _create_table_if_not_exists(self):
        """åˆ›å»ºç¼“å­˜è¡¨ï¼ˆå¦‚æžœä¸å­˜åœ¨ï¼‰"""
        try:
            self.db.execute(text("""
                CREATE TABLE IF NOT EXISTS batch_data_cache (
                    id SERIAL PRIMARY KEY,
                    compound_id VARCHAR(255) NOT NULL,
                    template_id VARCHAR(255) NOT NULL,
                    batch_data JSONB NOT NULL,
                    file_hashes TEXT[],
                    total_files INTEGER NOT NULL DEFAULT 0,
                    processed_files TEXT[],
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(compound_id, template_id)
                );
                
                CREATE INDEX IF NOT EXISTS idx_batch_cache_compound_template 
                ON batch_data_cache(compound_id, template_id);
            """))
            self.db.commit()
        except Exception as e:
            print(f"ç¼“å­˜è¡¨åˆ›å»ºè­¦å‘Š: {e}")
            self.db.rollback()

def get_cache_record(db: Session, compound_id: str, template_id: str) -> Optional[Dict]:
    """èŽ·å–ç¼“å­˜è®°å½•"""
    try:
        result = db.execute(
            text("""
            SELECT batch_data, updated_at, total_files, processed_files, file_hashes
            FROM batch_data_cache 
            WHERE compound_id = :compound_id AND template_id = :template_id
            """),
            {"compound_id": compound_id, "template_id": template_id}
        ).fetchone()
        
        if result:
            return {
                "batchData": result[0],  # JSONB data
                "lastUpdated": result[1].isoformat(),
                "totalFiles": result[2],
                "processedFiles": result[3] or [],
                "fileHashes": result[4] or []
            }
    except Exception as e:
        print(f"èŽ·å–ç¼“å­˜è®°å½•å¤±è´¥: {e}")
    return None

def update_cache_record(db: Session, compound_id: str, template_id: str, 
                       batch_data: List[Dict], file_hashes: List[str], 
                       processed_files: List[str]):
    """æ›´æ–°æˆ–åˆ›å»ºç¼“å­˜è®°å½•"""
    try:
        current_time = datetime.utcnow()
        
        # å…ˆå°è¯•æ›´æ–°
        result = db.execute(
            text("""
            UPDATE batch_data_cache 
            SET batch_data = :batch_data, file_hashes = :file_hashes, 
                total_files = :total_files, processed_files = :processed_files, 
                updated_at = :updated_at
            WHERE compound_id = :compound_id AND template_id = :template_id
            """),
            {
                "batch_data": json.dumps(batch_data),
                "file_hashes": file_hashes,
                "total_files": len(batch_data),
                "processed_files": processed_files,
                "updated_at": current_time,
                "compound_id": compound_id,
                "template_id": template_id
            }
        )
        
        # å¦‚æžœæ²¡æœ‰æ›´æ–°ä»»ä½•è¡Œï¼Œåˆ™æ’å…¥æ–°è®°å½•
        if result.rowcount == 0:
            db.execute(
                text("""
                INSERT INTO batch_data_cache 
                (compound_id, template_id, batch_data, file_hashes, total_files, 
                 processed_files, created_at, updated_at)
                VALUES (:compound_id, :template_id, :batch_data, :file_hashes, 
                        :total_files, :processed_files, :created_at, :updated_at)
                """),
                {
                    "compound_id": compound_id,
                    "template_id": template_id,
                    "batch_data": json.dumps(batch_data),
                    "file_hashes": file_hashes,
                    "total_files": len(batch_data),
                    "processed_files": processed_files,
                    "created_at": current_time,
                    "updated_at": current_time
                }
            )
        
        db.commit()
        return True
    except Exception as e:
        print(f"ç¼“å­˜æ›´æ–°å¤±è´¥: {e}")
        db.rollback()
        return False

def delete_cache_record(db: Session, compound_id: str, template_id: str) -> int:
    """åˆ é™¤ç¼“å­˜è®°å½•"""
    try:
        result = db.execute(
            text("""
            DELETE FROM batch_data_cache 
            WHERE compound_id = :compound_id AND template_id = :template_id
            """),
            {"compound_id": compound_id, "template_id": template_id}
        )
        db.commit()
        return result.rowcount
    except Exception as e:
        print(f"ç¼“å­˜åˆ é™¤å¤±è´¥: {e}")
        db.rollback()
        return 0


def calculate_file_hashes(pdf_directory: str) -> List[str]:
    """è®¡ç®—ç›®å½•ä¸­PDFæ–‡ä»¶çš„å“ˆå¸Œå€¼"""
    hashes = []
    try:
        pdf_files = glob.glob(os.path.join(pdf_directory, "*.pdf"))
        for pdf_file in pdf_files:
            filename = os.path.basename(pdf_file)
            # ä½¿ç”¨æ–‡ä»¶ä¿®æ”¹æ—¶é—´å’Œå¤§å°çš„ç»„åˆä½œä¸ºç®€å•å“ˆå¸Œ
            stat = os.stat(pdf_file)
            file_hash = f"{filename}:{stat.st_size}:{int(stat.st_mtime)}"
            hashes.append(file_hash)
    except Exception as e:
        print(f"è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¤±è´¥: {e}")
    return sorted(hashes)  # æŽ’åºä»¥ä¿è¯ä¸€è‡´æ€§

# ============ æ–°å¢žç¼“å­˜APIç«¯ç‚¹ ============

@router.get("/check-cache", response_model=ApiResponse)
async def check_cache(
    compound_id: str = Query(...),
    template_id: str = Query(...),
    db: Session = Depends(get_db)
):
    """æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å·²æœ‰æ‰¹æ¬¡æ•°æ®ç¼“å­˜"""
    try:
        # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        cache_manager = BatchDataCache(db)
        
        # èŽ·å–ç¼“å­˜è®°å½•
        cache_data = get_cache_record(db, compound_id, template_id)
        
        if cache_data:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰å˜æ›´
            pdf_directory = getattr(settings, 'PDF_DIRECTORY', settings.UPLOAD_DIR)
            current_hashes = calculate_file_hashes(pdf_directory)
            cached_hashes = cache_data.get("fileHashes", [])
            
            files_changed = set(current_hashes) != set(cached_hashes)
            
            if files_changed:
                return ApiResponse(
                    success=True,
                    data=None,
                    message="Files have changed since last cache, need reprocessing"
                )
            
            return ApiResponse(
                success=True,
                data=cache_data
            )
        else:
            return ApiResponse(
                success=True,
                data=None,
                message="No cache found"
            )
            
    except Exception as e:
        return ApiResponse(
            success=False,
            error=f"Failed to check cache: {str(e)}"
        )

@router.delete("/clear-cache", response_model=ApiResponse)
async def clear_cache(
    compound_id: str = Query(...),
    template_id: str = Query(...),
    db: Session = Depends(get_db)
):
    """æ¸…é™¤æŒ‡å®šåŒ–åˆç‰©å’Œæ¨¡æ¿çš„ç¼“å­˜æ•°æ®"""
    try:
        # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        cache_manager = BatchDataCache(db)
        
        # åˆ é™¤ç¼“å­˜è®°å½•
        deleted_count = delete_cache_record(db, compound_id, template_id)
        
        return ApiResponse(
            success=True,
            data={"deleted_count": deleted_count},
            message=f"Cleared {deleted_count} cache records"
        )
        
    except Exception as e:
        return ApiResponse(
            success=False,
            error=f"Failed to clear cache: {str(e)}"
        )

@router.get("/cache-status", response_model=ApiResponse)
async def get_cache_status(
    db: Session = Depends(get_db)
):
    """èŽ·å–ç¼“å­˜çŠ¶æ€ç»Ÿè®¡"""
    try:
        # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        cache_manager = BatchDataCache(db)
        
        # èŽ·å–ç¼“å­˜ç»Ÿè®¡
        try:
            result = db.execute(text("""
                SELECT 
                    COUNT(*) as total_records,
                    MAX(updated_at) as last_updated,
                    SUM(total_files) as total_files
                FROM batch_data_cache
            """)).fetchone()
            
            cache_records = db.execute(text("""
                SELECT compound_id, template_id, total_files, updated_at
                FROM batch_data_cache
                ORDER BY updated_at DESC
                LIMIT 10
            """)).fetchall()
            
            return ApiResponse(
                success=True,
                data={
                    "total_records": result[0] if result else 0,
                    "last_updated": result[1].isoformat() if result and result[1] else None,
                    "total_cached_files": result[2] if result else 0,
                    "recent_records": [
                        {
                            "compound_id": record[0],
                            "template_id": record[1],
                            "total_files": record[2],
                            "updated_at": record[3].isoformat()
                        } for record in cache_records
                    ]
                }
            )
        except Exception as sql_error:
            print(f"SQLæŸ¥è¯¢é”™è¯¯: {sql_error}")
            # å¦‚æžœè¡¨ä¸å­˜åœ¨ï¼Œè¿”å›žç©ºçŠ¶æ€
            return ApiResponse(
                success=True,
                data={
                    "total_records": 0,
                    "last_updated": None,
                    "total_cached_files": 0,
                    "recent_records": []
                }
            )
        
    except Exception as e:
        print(f"èŽ·å–ç¼“å­˜çŠ¶æ€å¤±è´¥: {e}")
        return ApiResponse(
            success=False,
            error=f"Failed to get cache status: {str(e)}"
        )

# ============ å¢žå¼ºçš„ä¸»è¦å¤„ç†ç«¯ç‚¹ ============

@router.post("/process-directory", response_model=ApiResponse)
async def process_directory(
    request: DirectoryProcessRequest,  # æ‰€æœ‰å‚æ•°éƒ½é€šè¿‡ JSON body ä¼ é€’
    db: Session = Depends(get_db)
):
    """Process all PDF files in the specified directory and extract batch analysis data"""
    try:
        # ä»Ž request å¯¹è±¡èŽ·å–å‚æ•°
        force_reprocess = getattr(request, 'force_reprocess', False)
        
        print(f"æ”¶åˆ°è¯·æ±‚:")
        print(f"  - compound_id: {request.compound_id}")
        print(f"  - template_id: {request.template_id}")
        print(f"  - force_reprocess: {force_reprocess}")
        
        # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        cache_manager = BatchDataCache(db)
        
        # ä½¿ç”¨é…ç½®ä¸­çš„PDFç›®å½•
        pdf_directory = getattr(settings, 'PDF_DIRECTORY', settings.UPLOAD_DIR)
        os.makedirs(pdf_directory, exist_ok=True)

        # å¦‚æžœç›®å½•ä¸ºç©ºï¼Œæä¾›ä¸Šä¼ æç¤º
        if not os.path.exists(pdf_directory):
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"PDF directory not found: {pdf_directory}. Please upload PDF files to this directory."
            )
        
        pdf_files = glob.glob(os.path.join(pdf_directory, "*.pdf"))
        
        if not pdf_files:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,  
                detail=f"No PDF files found in directory: {pdf_directory}. Please upload PDF files first."
            )
        
        # å¦‚æžœä¸æ˜¯å¼ºåˆ¶é‡æ–°å¤„ç†ï¼Œæ£€æŸ¥ç¼“å­˜
        if not force_reprocess:
            print(f"\nðŸ” Checking cache for compound: {request.compound_id}, template: {request.template_id}")
            cache_data = get_cache_record(db, request.compound_id, request.template_id)
            
            if cache_data:
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰å˜æ›´
                current_hashes = calculate_file_hashes(pdf_directory)
                cached_hashes = cache_data.get("fileHashes", [])
                
                if set(current_hashes) == set(cached_hashes):
                    # æ–‡ä»¶æ²¡æœ‰å˜æ›´ï¼Œè¿”å›žç¼“å­˜æ•°æ®
                    print(f"âœ… Cache hit! Loading {len(cache_data['batchData'])} batches from cache")
                    print(f"ðŸ“… Cache last updated: {cache_data['lastUpdated']}")
                    
                    return ApiResponse(
                        success=True,
                        data={
                            "processedFiles": cache_data.get("processedFiles", []),
                            "failedFiles": [],
                            "totalFiles": cache_data.get("totalFiles", 0),
                            "batchData": cache_data["batchData"],
                            "status": "success",
                            "fromCache": True,
                            "message": f"Loaded {len(cache_data['batchData'])} batches from cache (last updated: {cache_data['lastUpdated']})"
                        }
                    )
                else:
                    print(f"ðŸ“ Files have changed since last cache, proceeding with processing...")
            else:
                print(f"âŒ No cache found, proceeding with processing...")
        else:
            print(f"ðŸ”„ Force reprocess requested, skipping cache check...")
        
        # æ‰§è¡ŒåŽŸæœ‰çš„æ‰¹é‡PDFå¤„ç†é€»è¾‘
        print(f"\n{'='*80}")
        print(f"ðŸš€ COA BATCH ANALYSIS PROCESSING STARTED")
        print(f"{'='*80}")
        print(f"ðŸ“ Directory: {pdf_directory}")
        print(f"ðŸ“„ Found {len(pdf_files)} PDF files")
        print(f"ðŸ§¬ Compound ID: {request.compound_id}")
        print(f"ðŸ“‹ Template ID: {request.template_id}")
        
        # æ˜¾ç¤ºAIæœåŠ¡çŠ¶æ€
        if hasattr(settings, 'USE_AZURE_OPENAI') and settings.USE_AZURE_OPENAI:
            print(f"ðŸ”µ AI Service: Azure OpenAI ({getattr(settings, 'AZURE_OPENAI_DEPLOYMENT_NAME', 'Unknown')})")
        elif hasattr(settings, 'OPENAI_API_KEY') and settings.OPENAI_API_KEY:
            print(f"ðŸŸ¢ AI Service: Standard OpenAI")
        else:
            print(f"âš ï¸  AI Service: Not available")
        
        print(f"ðŸ§ª Test Parameters: {len(ai_extractor.get_test_parameters())} items")
        print(f"{'='*80}")
        
        batch_data_list = []
        processed_files = []
        failed_files = []
        
        for i, pdf_file in enumerate(pdf_files, 1):
            # ä¸ºæ¯ä¸ªæ–‡ä»¶åˆ›å»ºç‹¬ç«‹çš„äº‹åŠ¡
            document = None
            
            # ä½¿ç”¨å­äº‹åŠ¡æˆ–æ–°ä¼šè¯
            try:
                # å¼€å§‹æ–°äº‹åŠ¡
                with db.begin_nested():  # ä½¿ç”¨åµŒå¥—äº‹åŠ¡
                    filename = os.path.basename(pdf_file)
                    print(f"\nðŸ“„ Processing file {i}/{len(pdf_files)}: {filename}")
                    print("-" * 80)
                    
                    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
                    existing_doc = db.query(COADocument).filter(
                        COADocument.filename == filename,
                        COADocument.compound_id == UUID(request.compound_id)
                    ).first()
                    
                    if existing_doc:
                        print(f"âš ï¸  Document already exists: {filename}, skipping...")
                        continue
                    
                    # åˆ›å»ºæ•°æ®åº“è®°å½•
                    document = COADocument(
                        compound_id=UUID(request.compound_id),
                        filename=filename,
                        file_path=pdf_file,
                        file_size=f"{os.path.getsize(pdf_file) / 1024:.2f} KB",
                        processing_status=ProcessingStatus.PROCESSING.value
                    )
                    
                    db.add(document)
                    db.flush()  # èŽ·å–IDä½†ä¸æäº¤
                    
                    # æå–PDFæ–‡æœ¬
                    print("ðŸ“– Extracting text from PDF...")
                    pdf_text = await pdf_processor.extract_text(pdf_file)
                    
                    if not pdf_text.strip():
                        raise Exception("No text content found in PDF")
                    
                    print(f"âœ… Extracted {len(pdf_text)} characters of text")
                    
                    # ä½¿ç”¨AIæå–æ‰¹æ¬¡æ•°æ®
                    print("ðŸ” Extracting COA batch analysis data...")
                    batch_data = await ai_extractor.extract_coa_batch_data(pdf_text, filename)
                    
                    # éªŒè¯å’Œæ¸…ç†æ•°æ®
                    batch_data = ai_extractor.validate_batch_data(batch_data)
                    
                    # æ›´æ–°æ–‡æ¡£çŠ¶æ€
                    document.processing_status = ProcessingStatus.COMPLETED.value
                    document.processed_at = datetime.utcnow()
                    
                    # ä¿å­˜æå–çš„æ‰¹æ¬¡æ•°æ®
                    batch_number = batch_data.get('batch_number', '')
                    manufacture_date = batch_data.get('manufacture_date', '')
                    manufacturer = batch_data.get('manufacturer', '')
                    
                    # ä¿å­˜åŸºæœ¬æ‰¹æ¬¡ä¿¡æ¯
                    basic_fields = [
                        ('batch_number', batch_number),
                        ('manufacture_date', manufacture_date),
                        ('manufacturer', manufacturer)
                    ]
                    
                    for field_name, field_value in basic_fields:
                        if field_value:
                            data = ExtractedData(
                                document_id=document.id,
                                field_name=field_name,
                                field_value=field_value,
                                confidence_score=0.95,
                                original_text=field_value
                            )
                            db.add(data)
                    
                    # ä¿å­˜æµ‹è¯•ç»“æžœæ•°æ®
                    test_results = batch_data.get('test_results', {})
                    for test_param, result_value in test_results.items():
                        if result_value and result_value not in ['TBD', '']:
                            data = ExtractedData(
                                document_id=document.id,
                                field_name=test_param,
                                field_value=result_value,
                                confidence_score=0.90,
                                original_text=result_value
                            )
                            db.add(data)
                
                # æäº¤åµŒå¥—äº‹åŠ¡
                db.commit()
                
                # æ·»åŠ åˆ°æˆåŠŸåˆ—è¡¨
                batch_data_list.append(batch_data)
                processed_files.append(filename)
                
                # æ˜¾ç¤ºå¤„ç†æ‘˜è¦
                print(f"\nâœ… Successfully processed: {filename}")
                print(f"ðŸ“¦ Batch: {batch_number}")
                print(f"ðŸ“… Date: {manufacture_date}")
                print(f"ðŸ§ª Test Results: {len([v for v in test_results.values() if v not in ['TBD', 'ND', '']])}/{len(test_results)}")
                
            except Exception as e:
                # å›žæ»šåµŒå¥—äº‹åŠ¡
                db.rollback()
                
                error_msg = str(e)
                filename = os.path.basename(pdf_file) if 'pdf_file' in locals() else "unknown"
                print(f"âŒ Error processing {filename}: {error_msg}")
                failed_files.append({"filename": filename, "error": error_msg})
                
                # å¦‚æžœæ–‡æ¡£å·²åˆ›å»ºï¼Œå°è¯•æ›´æ–°çŠ¶æ€ä¸ºå¤±è´¥
                if document and document.id:
                    try:
                        # ä½¿ç”¨æ–°çš„åµŒå¥—äº‹åŠ¡æ¥æ›´æ–°å¤±è´¥çŠ¶æ€
                        with db.begin_nested():
                            fail_doc = db.query(COADocument).filter(
                                COADocument.id == document.id
                            ).first()
                            if fail_doc:
                                fail_doc.processing_status = ProcessingStatus.FAILED.value
                                fail_doc.error_message = error_msg[:500]  # é™åˆ¶é”™è¯¯æ¶ˆæ¯é•¿åº¦
                        db.commit()
                    except Exception as update_error:
                        print(f"Failed to update document status: {update_error}")
                        db.rollback()
                
                continue
        
        # å¦‚æžœå¤„ç†æˆåŠŸï¼Œæ›´æ–°ç¼“å­˜
        if batch_data_list:
            try:
                print(f"\nðŸ’¾ Updating cache...")
                current_hashes = calculate_file_hashes(pdf_directory)
                cache_updated = update_cache_record(
                    db, request.compound_id, request.template_id, 
                    batch_data_list, current_hashes, processed_files
                )
                if cache_updated:
                    print(f"âœ… Cache updated successfully")
                else:
                    print(f"âš ï¸ Cache update failed")
            except Exception as e:
                print(f"âš ï¸ Cache update error: {e}")
        
        # å¤„ç†å®ŒæˆåŽçš„æ±‡æ€»
        print(f"\n{'='*80}")
        print(f"ðŸ“ˆ BATCH ANALYSIS PROCESSING SUMMARY")
        print(f"{'='*80}")
        print(f"ðŸ“„ Total files found: {len(pdf_files)}")
        print(f"âœ… Successfully processed: {len(processed_files)}")
        print(f"âŒ Failed: {len(failed_files)}")
        print(f"ðŸ“Š Total batches analyzed: {len(batch_data_list)}")
        
        if failed_files:
            print(f"\nâŒ Failed files:")
            for failed in failed_files:
                print(f"   â€¢ {failed['filename']}: {failed['error']}")
        
        if batch_data_list:
            print(f"\nâœ… Successfully processed batches:")
            for batch_data in batch_data_list:
                batch_num = batch_data.get('batch_number', 'Unknown')
                mfg_date = batch_data.get('manufacture_date', 'Unknown')
                test_count = len([v for v in batch_data.get('test_results', {}).values() if v not in ['TBD', 'ND', '']])
                print(f"   â€¢ {batch_num} (Mfg: {mfg_date}) - {test_count} test results")
        
        print(f"\nðŸ”„ Data Structure:")
        print(f"   â€¢ Each batch maintains independent data (no merging)")
        print(f"   â€¢ Ready for table generation with {len(batch_data_list)} columns")
        print(f"   â€¢ Test parameters: {len(ai_extractor.get_test_parameters())} items")
        print(f"{'='*80}")
        
        # å‡†å¤‡è¿”å›žæ•°æ®ï¼ˆä¿æŒæ¯ä¸ªæ‰¹æ¬¡ç‹¬ç«‹ï¼‰
        return ApiResponse(
            success=True,
            data={
                "processedFiles": processed_files,
                "failedFiles": failed_files,
                "totalFiles": len(pdf_files),
                "batchData": batch_data_list,  # æ‰¹æ¬¡æ•°æ®åˆ—è¡¨ï¼Œä¸åˆå¹¶
                "status": "success" if processed_files else "failed",
                "fromCache": False,
                "cacheUpdated": len(batch_data_list) > 0,
                "message": f"Successfully processed {len(batch_data_list)} batches for COA analysis"
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        error_msg = str(e)
        print(f"\nâŒ Directory processing failed: {error_msg}")
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=error_msg
        )

# ============ ä¿ç•™åŽŸæœ‰çš„å…¶ä»–ç«¯ç‚¹ ============

@router.get("/batch-analysis/{compound_id}", response_model=ApiResponse)
async def get_batch_analysis_data(
    compound_id: UUID,
    db: Session = Depends(get_db)
):
    """Get all batch analysis data for a compound"""
    try:
        # èŽ·å–è¯¥åŒ–åˆç‰©çš„æ‰€æœ‰æ–‡æ¡£
        documents = db.query(COADocument).filter(
            COADocument.compound_id == compound_id,
            COADocument.processing_status == ProcessingStatus.COMPLETED.value
        ).all()
        
        if not documents:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"No processed documents found for compound {compound_id}"
            )
        
        batch_data_list = []
        
        for document in documents:
            # èŽ·å–è¯¥æ–‡æ¡£çš„æ‰€æœ‰æå–æ•°æ®
            extracted_data = db.query(ExtractedData).filter(
                ExtractedData.document_id == document.id
            ).all()
            
            if extracted_data:
                # é‡æž„æ‰¹æ¬¡æ•°æ®
                batch_data = {
                    "filename": document.filename,
                    "batch_number": "",
                    "manufacture_date": "",
                    "manufacturer": "",
                    "test_results": {}
                }
                
                for data in extracted_data:
                    if data.field_name == "batch_number":
                        batch_data["batch_number"] = data.field_value
                    elif data.field_name == "manufacture_date":
                        batch_data["manufacture_date"] = data.field_value
                    elif data.field_name == "manufacturer":
                        batch_data["manufacturer"] = data.field_value
                    else:
                        batch_data["test_results"][data.field_name] = data.field_value
                
                batch_data_list.append(batch_data)
        
        return ApiResponse(
            success=True,
            data={
                "compoundId": str(compound_id),
                "totalBatches": len(batch_data_list),
                "batchData": batch_data_list
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )

@router.post("/upload", response_model=ApiResponse)
async def upload_document(
    file: UploadFile = File(...),
    compound_id: str = Form(...),
    template_id: Optional[str] = Form(None),
    db: Session = Depends(get_db)
):
    """Upload a COA document (legacy endpoint)"""
    try:
        if not file.filename.endswith('.pdf'):
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Only PDF files are allowed"
            )
        
        if file.size > settings.MAX_FILE_SIZE:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"File size exceeds maximum allowed size of {settings.MAX_FILE_SIZE / 1024 / 1024}MB"
            )
        
        file_path = await file_manager.save_upload(file, compound_id)
        
        document = COADocument(
            compound_id=UUID(compound_id),
            filename=file.filename,
            file_path=file_path,
            file_size=f"{file.size / 1024:.2f} KB",
            processing_status=ProcessingStatus.PENDING.value
        )
        
        db.add(document)
        db.commit()
        db.refresh(document)
        
        return ApiResponse(
            success=True,
            data={
                "documentId": str(document.id),
                "filename": document.filename,
                "status": document.processing_status
            }
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )