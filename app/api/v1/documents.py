# app/api/v1/documents.py - Enhanced with Veeva Integration
from fastapi import APIRouter, Depends, HTTPException, status, File, UploadFile, Form, Query
from sqlalchemy.orm import Session
from typing import Optional, List, Dict, Any
from uuid import UUID
from sqlalchemy import text
import os
import glob
import aiofiles
from datetime import datetime
import hashlib
import json
import requests
from pathlib import Path
import io
import gc  # For garbage collection
import asyncio
from concurrent.futures import ThreadPoolExecutor
import logging

from ...database import get_db
from ...config import settings
from ...models.document import COADocument, ProcessingStatus
from ...models.extracted_data import ExtractedData
from ...schemas.document import (
    DirectoryProcessRequest,
    VeevaProcessRequest  # New schema for Veeva processing
)
from ...schemas.extracted_data import ProcessingResultResponse, ApiResponse
from ...services.file_manager import FileManager
from ...services.pdf_processor import PDFProcessor
from ...services.ai_extractor import AIExtractor
from ...services.veeva_service import VeevaService, VeevaAPIError  # New Veeva service

router = APIRouter()
logger = logging.getLogger(__name__)

# Initialize services
file_manager = FileManager(settings.UPLOAD_DIR)
pdf_processor = PDFProcessor()
ai_extractor = AIExtractor()

# ============ ÁºìÂ≠òÁõ∏ÂÖ≥ÂáΩÊï∞ ============

class BatchDataCache:
    """ÊâπÊ¨°Êï∞ÊçÆÁºìÂ≠òÁÆ°ÁêÜÂô®"""
    def __init__(self, db: Session):
        self.db = db
        self._create_table_if_not_exists()
    
    def _create_table_if_not_exists(self):
        """ÂàõÂª∫ÁºìÂ≠òË°®ÔºàÂ¶ÇÊûú‰∏çÂ≠òÂú®Ôºâ"""
        try:
            self.db.execute(text("""
                CREATE TABLE IF NOT EXISTS coa_processor.batch_data_cache (
                    id SERIAL PRIMARY KEY,
                    compound_id VARCHAR(255) NOT NULL,
                    template_id VARCHAR(255) NOT NULL,
                    batch_data JSONB NOT NULL,
                    file_hashes TEXT[],
                    total_files INTEGER NOT NULL DEFAULT 0,
                    processed_files TEXT[],
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(compound_id, template_id)
                );
                
                CREATE INDEX IF NOT EXISTS idx_batch_cache_compound_template 
                ON coa_processor.batch_data_cache(compound_id, template_id);
            """))
            self.db.commit()
        except Exception as e:
            print(f"ÁºìÂ≠òË°®ÂàõÂª∫Ë≠¶Âëä: {e}")
            self.db.rollback()

def get_cache_record(db: Session, compound_id: str, template_id: str) -> Optional[Dict]:
    """Ëé∑ÂèñÁºìÂ≠òËÆ∞ÂΩï"""
    try:
        result = db.execute(
            text("""
            SELECT batch_data, updated_at, total_files, processed_files, file_hashes
            FROM coa_processor.batch_data_cache 
            WHERE compound_id = :compound_id AND template_id = :template_id
            """),
            {"compound_id": compound_id, "template_id": template_id}
        ).fetchone()
        
        if result:
            return {
                "batchData": result[0],  # JSONB data
                "lastUpdated": result[1].isoformat(),
                "totalFiles": result[2],
                "processedFiles": result[3] or [],
                "fileHashes": result[4] or []
            }
    except Exception as e:
        print(f"Ëé∑ÂèñÁºìÂ≠òËÆ∞ÂΩïÂ§±Ë¥•: {e}")
    return None

def update_cache_record(db: Session, compound_id: str, template_id: str, 
                       batch_data: List[Dict], file_hashes: List[str], 
                       processed_files: List[str]):
    """Êõ¥Êñ∞ÊàñÂàõÂª∫ÁºìÂ≠òËÆ∞ÂΩï"""
    try:
        current_time = datetime.utcnow()
        
        # ÂÖàÂ∞ùËØïÊõ¥Êñ∞
        result = db.execute(
            text("""
            UPDATE coa_processor.batch_data_cache 
            SET batch_data = :batch_data, file_hashes = :file_hashes, 
                total_files = :total_files, processed_files = :processed_files, 
                updated_at = :updated_at
            WHERE compound_id = :compound_id AND template_id = :template_id
            """),
            {
                "batch_data": json.dumps(batch_data),
                "file_hashes": file_hashes,
                "total_files": len(batch_data),
                "processed_files": processed_files,
                "updated_at": current_time,
                "compound_id": compound_id,
                "template_id": template_id
            }
        )
        
        # Â¶ÇÊûúÊ≤°ÊúâÊõ¥Êñ∞‰ªª‰ΩïË°åÔºåÂàôÊèíÂÖ•Êñ∞ËÆ∞ÂΩï
        if result.rowcount == 0:
            db.execute(
                text("""
                INSERT INTO coa_processor.batch_data_cache 
                (compound_id, template_id, batch_data, file_hashes, total_files, 
                 processed_files, created_at, updated_at)
                VALUES (:compound_id, :template_id, :batch_data, :file_hashes, 
                        :total_files, :processed_files, :created_at, :updated_at)
                """),
                {
                    "compound_id": compound_id,
                    "template_id": template_id,
                    "batch_data": json.dumps(batch_data),
                    "file_hashes": file_hashes,
                    "total_files": len(batch_data),
                    "processed_files": processed_files,
                    "created_at": current_time,
                    "updated_at": current_time
                }
            )
        
        db.commit()
        return True
    except Exception as e:
        print(f"ÁºìÂ≠òÊõ¥Êñ∞Â§±Ë¥•: {e}")
        db.rollback()
        return False

def delete_cache_record(db: Session, compound_id: str, template_id: str) -> int:
    """Âà†Èô§ÁºìÂ≠òËÆ∞ÂΩï"""
    try:
        result = db.execute(
            text("""
            DELETE FROM coa_processor.batch_data_cache 
            WHERE compound_id = :compound_id AND template_id = :template_id
            """),
            {"compound_id": compound_id, "template_id": template_id}
        )
        db.commit()
        return result.rowcount
    except Exception as e:
        print(f"ÁºìÂ≠òÂà†Èô§Â§±Ë¥•: {e}")
        db.rollback()
        return 0


def calculate_file_hashes(pdf_directory: str) -> List[str]:
    """ËÆ°ÁÆóÁõÆÂΩï‰∏≠PDFÊñá‰ª∂ÁöÑÂìàÂ∏åÂÄº"""
    hashes = []
    try:
        pdf_files = glob.glob(os.path.join(pdf_directory, "*.pdf"))
        for pdf_file in pdf_files:
            filename = os.path.basename(pdf_file)
            # ‰ΩøÁî®Êñá‰ª∂‰øÆÊîπÊó∂Èó¥ÂíåÂ§ßÂ∞èÁöÑÁªÑÂêà‰Ωú‰∏∫ÁÆÄÂçïÂìàÂ∏å
            stat = os.stat(pdf_file)
            file_hash = f"{filename}:{stat.st_size}:{int(stat.st_mtime)}"
            hashes.append(file_hash)
    except Exception as e:
        print(f"ËÆ°ÁÆóÊñá‰ª∂ÂìàÂ∏åÂ§±Ë¥•: {e}")
    return sorted(hashes)  # ÊéíÂ∫è‰ª•‰øùËØÅ‰∏ÄËá¥ÊÄß





# ============ NEW: Veeva Integration Functions ============

async def process_veeva_document_stream(
    pdf_stream: io.BytesIO,
    document_id: str,
    metadata: Dict,
    compound_id: UUID,
    db: Session
) -> Dict:
    """
    Process a single PDF document from Veeva stream
    
    Args:
        pdf_stream: BytesIO containing PDF data
        document_id: Veeva document ID
        metadata: Document metadata from Veeva
        compound_id: Compound ID for database
        db: Database session
    
    Returns:
        Dictionary with processing results
    """
    document = None
    
    try:
        # Create filename from metadata
        doc_name = metadata.get('name__v', document_id)
        doc_version = f"{metadata.get('major_version_number__v', 1)}.{metadata.get('minor_version_number__v', 0)}"
        filename = f"{doc_name}_v{doc_version}.pdf"
        
        logger.info(f"üìÑ Processing Veeva document: {filename} (ID: {document_id})")
        
        # Check if document already exists
        existing_doc = db.query(COADocument).filter(
            COADocument.filename == filename,
            COADocument.compound_id == compound_id
        ).first()
        
        if existing_doc:
            logger.warning(f"‚ö†Ô∏è Document already exists: {filename}, skipping...")
            return {
                "status": "skipped",
                "filename": filename,
                "document_id": document_id,
                "message": "Document already processed"
            }
        
        # Create database record
        document = COADocument(
            compound_id=compound_id,
            filename=filename,
            file_path=f"veeva://{document_id}",  # Virtual path for Veeva documents
            file_size=f"{metadata.get('downloaded_size', 0) / 1024:.2f} KB",
            processing_status=ProcessingStatus.PROCESSING.value
        )
        
        db.add(document)
        db.flush()  # Get ID but don't commit yet
        
        # Extract PDF text from stream
        logger.info("üìñ Extracting text from PDF stream...")
        pdf_text = await pdf_processor.extract_text_from_stream(pdf_stream)
        
        if not pdf_text.strip():
            raise Exception("No text content found in PDF")
        
        logger.info(f"‚úÖ Extracted {len(pdf_text)} characters of text")
        
        # Use AI to extract batch data
        logger.info("üîç Extracting COA batch analysis data...")
        batch_data = await ai_extractor.extract_coa_batch_data(pdf_text, filename)
        
        # Validate and clean data
        batch_data = ai_extractor.validate_batch_data(batch_data)
        
        # Update document status
        document.processing_status = ProcessingStatus.COMPLETED.value
        document.processed_at = datetime.utcnow()
        
        # Save extracted batch data
        batch_number = batch_data.get('batch_number', '')
        manufacture_date = batch_data.get('manufacture_date', '')
        manufacturer = batch_data.get('manufacturer', '')
        
        # Save basic batch information
        basic_fields = [
            ('batch_number', batch_number),
            ('manufacture_date', manufacture_date),
            ('manufacturer', manufacturer)
        ]
        
        for field_name, field_value in basic_fields:
            if field_value:
                data = ExtractedData(
                    document_id=document.id,
                    field_name=field_name,
                    field_value=field_value,
                    confidence_score=0.95,
                    original_text=field_value
                )
                db.add(data)
        
        # Save test results data
        test_results = batch_data.get('test_results', {})
        for test_param, result_value in test_results.items():
            if result_value and result_value not in ['TBD', '']:
                data = ExtractedData(
                    document_id=document.id,
                    field_name=test_param,
                    field_value=result_value,
                    confidence_score=0.90,
                    original_text=result_value
                )
                db.add(data)
        
        # Commit transaction
        db.commit()
        
        # Add Veeva-specific metadata to batch data
        batch_data['veeva_document_id'] = document_id
        batch_data['veeva_version'] = doc_version
        
        logger.info(f"‚úÖ Successfully processed Veeva document: {filename}")
        
        return {
            "status": "success",
            "filename": filename,
            "document_id": document_id,
            "batch_data": batch_data
        }
        
    except Exception as e:
        # Rollback transaction
        db.rollback()
        
        error_msg = str(e)
        logger.error(f"‚ùå Error processing Veeva document {document_id}: {error_msg}")
        
        # Update document status if it was created
        if document and document.id:
            try:
                fail_doc = db.query(COADocument).filter(
                    COADocument.id == document.id
                ).first()
                if fail_doc:
                    fail_doc.processing_status = ProcessingStatus.FAILED.value
                    fail_doc.error_message = error_msg[:500]
                    db.commit()
            except Exception as update_error:
                logger.error(f"Failed to update document status: {update_error}")
                db.rollback()
        
        return {
            "status": "failed",
            "filename": filename if 'filename' in locals() else document_id,
            "document_id": document_id,
            "error": error_msg
        }
    finally:
        # Clean up memory
        if pdf_stream:
            pdf_stream.close()
            del pdf_stream
        gc.collect()

# ============ NEW: Veeva Processing Endpoint ============

@router.post("/process-from-veeva", response_model=ApiResponse)
async def process_from_veeva(
    request: VeevaProcessRequest,
    db: Session = Depends(get_db)
):
    """
    Process COA documents directly from Veeva Vault
    
    This endpoint:
    1. Receives a list of Veeva document IDs
    2. Downloads each document as a stream from Veeva
    3. Processes them in memory without saving to disk
    4. Extracts batch analysis data
    5. Stores results in database
    """
    
    if not settings.VEEVA_ENABLED:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="Veeva integration is not enabled"
        )
    
    try:
        print(f"\n{'='*80}")
        print(f"üöÄ VEEVA COA BATCH ANALYSIS PROCESSING STARTED")
        print(f"{'='*80}")
        print(f"üìã Document IDs: {request.document_ids}")
        print(f"üß¨ Compound ID: {request.compound_id}")
        print(f"üìã Template ID: {request.template_id}")
        print(f"üîÑ Force reprocess: {request.force_reprocess}")
        
        # Initialize cache manager
        cache_manager = BatchDataCache(db)
        
        # Check cache if not forcing reprocess
        if not request.force_reprocess and settings.ENABLE_CACHE:
            # Create a hash of document IDs for cache key
            doc_ids_hash = hashlib.md5(
                ",".join(sorted(request.document_ids)).encode()
            ).hexdigest()
            
            cache_data = get_cache_record(db, request.compound_id, request.template_id)
            
            if cache_data:
                # Check if same documents
                cached_hashes = cache_data.get("fileHashes", [])
                if doc_ids_hash in cached_hashes:
                    print(f"‚úÖ Cache hit! Returning cached data")
                    return ApiResponse(
                        success=True,
                        data={
                            "processedFiles": cache_data.get("processedFiles", []),
                            "failedFiles": [],
                            "totalFiles": cache_data.get("totalFiles", 0),
                            "batchData": cache_data["batchData"],
                            "status": "success",
                            "fromCache": True,
                            "source": "veeva",
                            "message": f"Loaded from cache (last updated: {cache_data['lastUpdated']})"
                        }
                    )
        
        # Initialize Veeva service
        veeva_service = VeevaService()
        
        # Test connection
        if not veeva_service.test_connection():
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="Failed to connect to Veeva Vault"
            )
        
        print(f"‚úÖ Connected to Veeva Vault successfully")
        
        batch_data_list = []
        processed_files = []
        failed_files = []
        
        # Process documents concurrently
        tasks = []
        
        for doc_id in request.document_ids:
            try:
                print(f"\nüì• Downloading from Veeva: {doc_id}")
                
                # Download document from Veeva
                pdf_stream, metadata = veeva_service.download_document_as_stream(doc_id)
                
                # Process the document stream
                result = await process_veeva_document_stream(
                    pdf_stream=pdf_stream,
                    document_id=doc_id,
                    metadata=metadata,
                    compound_id=UUID(request.compound_id),
                    db=db
                )
                
                if result["status"] == "success":
                    batch_data_list.append(result["batch_data"])
                    processed_files.append(result["filename"])
                elif result["status"] == "skipped":
                    print(f"‚è≠Ô∏è Skipped: {result['message']}")
                else:
                    failed_files.append({
                        "document_id": doc_id,
                        "filename": result.get("filename", doc_id),
                        "error": result.get("error", "Unknown error")
                    })
                    
            except VeevaAPIError as e:
                logger.error(f"‚ùå Veeva API error for {doc_id}: {e}")
                failed_files.append({
                    "document_id": doc_id,
                    "filename": doc_id,
                    "error": str(e)
                })
            except Exception as e:
                logger.error(f"‚ùå Unexpected error for {doc_id}: {e}")
                failed_files.append({
                    "document_id": doc_id,
                    "filename": doc_id,
                    "error": str(e)
                })
        
        # Close Veeva service
        veeva_service.close()
        
        # Update cache if processing was successful
        if batch_data_list and settings.ENABLE_CACHE:
            try:
                doc_ids_hash = hashlib.md5(
                    ",".join(sorted(request.document_ids)).encode()
                ).hexdigest()
                
                update_cache_record(
                    db, request.compound_id, request.template_id,
                    batch_data_list, [doc_ids_hash], processed_files
                )
                print(f"‚úÖ Cache updated successfully")
            except Exception as e:
                print(f"‚ö†Ô∏è Cache update failed: {e}")
        
        # Summary
        print(f"\n{'='*80}")
        print(f"üìà VEEVA PROCESSING SUMMARY")
        print(f"{'='*80}")
        print(f"üìÑ Total documents requested: {len(request.document_ids)}")
        print(f"‚úÖ Successfully processed: {len(processed_files)}")
        print(f"‚ùå Failed: {len(failed_files)}")
        print(f"üìä Total batches analyzed: {len(batch_data_list)}")
        
        if failed_files:
            print(f"\n‚ùå Failed documents:")
            for failed in failed_files:
                print(f"   ‚Ä¢ {failed['document_id']}: {failed['error']}")
        
        return ApiResponse(
            success=True,
            data={
                "processedFiles": processed_files,
                "failedFiles": failed_files,
                "totalFiles": len(request.document_ids),
                "batchData": batch_data_list,
                "status": "success" if processed_files else "failed",
                "fromCache": False,
                "source": "veeva",
                "message": f"Processed {len(batch_data_list)} batches from Veeva"
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        error_msg = str(e)
        logger.error(f"‚ùå Veeva processing failed: {error_msg}")
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=error_msg
        )

# ============ Hybrid Processing Endpoint ============

@router.post("/process-hybrid", response_model=ApiResponse)
async def process_hybrid(
    request: Dict[str, Any],
    db: Session = Depends(get_db)
):
    """
    Hybrid processing endpoint that can handle both local files and Veeva documents
    
    Request format:
    {
        "compound_id": "uuid",
        "template_id": "uuid",
        "source": "local" | "veeva" | "auto",
        "document_ids": ["VV-QDOC-xxx", ...],  // For Veeva
        "directory_path": "/path/to/pdfs",     // For local
        "force_reprocess": false
    }
    """
    
    source = request.get("source", "auto")
    
    if source == "veeva" or (source == "auto" and request.get("document_ids")):
        # Process from Veeva
        veeva_request = VeevaProcessRequest(
            compound_id=request["compound_id"],
            template_id=request["template_id"],
            document_ids=request["document_ids"],
            force_reprocess=request.get("force_reprocess", False)
        )
        return await process_from_veeva(veeva_request, db)
        
    elif source == "local" or (source == "auto" and request.get("directory_path")):
        # Process from local directory
        dir_request = DirectoryProcessRequest(
            compound_id=request["compound_id"],
            template_id=request["template_id"],
            directory_path=request.get("directory_path"),
            force_reprocess=request.get("force_reprocess", False)
        )
        return await process_directory(dir_request, db)
        
    else:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid request: must specify either document_ids for Veeva or directory_path for local processing"
        )


# ============ Êñ∞Â¢ûÁºìÂ≠òAPIÁ´ØÁÇπ ============

@router.get("/check-cache", response_model=ApiResponse)
async def check_cache(
    compound_id: str = Query(...),
    template_id: str = Query(...),
    db: Session = Depends(get_db)
):
    """Ê£ÄÊü•Êï∞ÊçÆÂ∫ì‰∏≠ÊòØÂê¶Â∑≤ÊúâÊâπÊ¨°Êï∞ÊçÆÁºìÂ≠ò"""
    try:
        # ÂàùÂßãÂåñÁºìÂ≠òÁÆ°ÁêÜÂô®
        cache_manager = BatchDataCache(db)
        
        # Ëé∑ÂèñÁºìÂ≠òËÆ∞ÂΩï
        cache_data = get_cache_record(db, compound_id, template_id)
        
        if cache_data:
            # Ê£ÄÊü•Êñá‰ª∂ÊòØÂê¶ÊúâÂèòÊõ¥
            pdf_directory = getattr(settings, 'PDF_DIRECTORY', settings.UPLOAD_DIR)
            current_hashes = calculate_file_hashes(pdf_directory)
            cached_hashes = cache_data.get("fileHashes", [])
            
            files_changed = set(current_hashes) != set(cached_hashes)
            
            if files_changed:
                return ApiResponse(
                    success=True,
                    data=None,
                    message="Files have changed since last cache, need reprocessing"
                )
            
            return ApiResponse(
                success=True,
                data=cache_data
            )
        else:
            return ApiResponse(
                success=True,
                data=None,
                message="No cache found"
            )
            
    except Exception as e:
        return ApiResponse(
            success=False,
            error=f"Failed to check cache: {str(e)}"
        )

@router.delete("/clear-cache", response_model=ApiResponse)
async def clear_cache(
    compound_id: str = Query(...),
    template_id: str = Query(...),
    db: Session = Depends(get_db)
):
    """Ê∏ÖÈô§ÊåáÂÆöÂåñÂêàÁâ©ÂíåÊ®°ÊùøÁöÑÁºìÂ≠òÊï∞ÊçÆ"""
    try:
        # ÂàùÂßãÂåñÁºìÂ≠òÁÆ°ÁêÜÂô®
        cache_manager = BatchDataCache(db)
        
        # Âà†Èô§ÁºìÂ≠òËÆ∞ÂΩï
        deleted_count = delete_cache_record(db, compound_id, template_id)
        
        return ApiResponse(
            success=True,
            data={"deleted_count": deleted_count},
            message=f"Cleared {deleted_count} cache records"
        )
        
    except Exception as e:
        return ApiResponse(
            success=False,
            error=f"Failed to clear cache: {str(e)}"
        )

@router.get("/cache-status", response_model=ApiResponse)
async def get_cache_status(
    db: Session = Depends(get_db)
):
    """Ëé∑ÂèñÁºìÂ≠òÁä∂ÊÄÅÁªüËÆ°"""
    try:
        # ÂàùÂßãÂåñÁºìÂ≠òÁÆ°ÁêÜÂô®
        cache_manager = BatchDataCache(db)
        
        # Ëé∑ÂèñÁºìÂ≠òÁªüËÆ°
        try:
            result = db.execute(text("""
                SELECT 
                    COUNT(*) as total_records,
                    MAX(updated_at) as last_updated,
                    SUM(total_files) as total_files
                FROM coa_processor.batch_data_cache
            """)).fetchone()
            
            cache_records = db.execute(text("""
                SELECT compound_id, template_id, total_files, updated_at
                FROM coa_processor.batch_data_cache
                ORDER BY updated_at DESC
                LIMIT 10
            """)).fetchall()
            
            return ApiResponse(
                success=True,
                data={
                    "total_records": result[0] if result else 0,
                    "last_updated": result[1].isoformat() if result and result[1] else None,
                    "total_cached_files": result[2] if result else 0,
                    "recent_records": [
                        {
                            "compound_id": record[0],
                            "template_id": record[1],
                            "total_files": record[2],
                            "updated_at": record[3].isoformat()
                        } for record in cache_records
                    ]
                }
            )
        except Exception as sql_error:
            print(f"SQLÊü•ËØ¢ÈîôËØØ: {sql_error}")
            # Â¶ÇÊûúË°®‰∏çÂ≠òÂú®ÔºåËøîÂõûÁ©∫Áä∂ÊÄÅ
            return ApiResponse(
                success=True,
                data={
                    "total_records": 0,
                    "last_updated": None,
                    "total_cached_files": 0,
                    "recent_records": []
                }
            )
        
    except Exception as e:
        print(f"Ëé∑ÂèñÁºìÂ≠òÁä∂ÊÄÅÂ§±Ë¥•: {e}")
        return ApiResponse(
            success=False,
            error=f"Failed to get cache status: {str(e)}"
        )
    
@router.get("/debug-directory", response_model=ApiResponse)
async def debug_directory_info(
    compound_id: str = Query(...),
    template_id: str = Query(...),
    db: Session = Depends(get_db)
):
    """Ë∞ÉËØïÁ´ØÁÇπÔºöÊ£ÄÊü•PDFÁõÆÂΩïÂíåÊñá‰ª∂Áä∂ÊÄÅ"""
    try:
        debug_info = {}
        
        # 1. Ê£ÄÊü•PDFÁõÆÂΩïÈÖçÁΩÆ
        pdf_directory = getattr(settings, 'PDF_DIRECTORY', settings.UPLOAD_DIR)
        debug_info['pdf_directory'] = pdf_directory
        debug_info['directory_exists'] = os.path.exists(pdf_directory)
        
        # 2. Ê£ÄÊü•ÁõÆÂΩïÂÜÖÂÆπ
        if os.path.exists(pdf_directory):
            all_files = os.listdir(pdf_directory)
            pdf_files = glob.glob(os.path.join(pdf_directory, "*.pdf"))
            debug_info['all_files'] = all_files
            debug_info['pdf_files'] = [os.path.basename(f) for f in pdf_files]
            debug_info['pdf_count'] = len(pdf_files)
            
            # 3. Ê£ÄÊü•PDFÊñá‰ª∂ËØ¶ÊÉÖ
            pdf_details = []
            for pdf_file in pdf_files[:3]:  # Âè™Ê£ÄÊü•Ââç3‰∏™Êñá‰ª∂
                try:
                    stat = os.stat(pdf_file)
                    # Â∞ùËØïÊèêÂèñÊñáÊú¨
                    text_sample = await pdf_processor.extract_text(pdf_file)
                    
                    pdf_details.append({
                        'filename': os.path.basename(pdf_file),
                        'size_kb': round(stat.st_size / 1024, 2),
                        'modified': datetime.fromtimestamp(stat.st_mtime).isoformat(),
                        'text_length': len(text_sample),
                        'text_preview': text_sample[:200] + "..." if len(text_sample) > 200 else text_sample,
                        'can_extract_text': len(text_sample.strip()) > 0
                    })
                except Exception as e:
                    pdf_details.append({
                        'filename': os.path.basename(pdf_file),
                        'error': str(e)
                    })
            
            debug_info['pdf_details'] = pdf_details
        else:
            debug_info['all_files'] = []
            debug_info['pdf_files'] = []
            debug_info['pdf_count'] = 0
            debug_info['pdf_details'] = []
        
        # 4. Ê£ÄÊü•AIÊúçÂä°ÈÖçÁΩÆ
        ai_config = {
            'use_azure_openai': getattr(settings, 'USE_AZURE_OPENAI', False),
            'azure_deployment': getattr(settings, 'AZURE_OPENAI_DEPLOYMENT_NAME', None),
            'has_openai_key': bool(getattr(settings, 'OPENAI_API_KEY', None)),
            'test_parameters_count': len(ai_extractor.get_test_parameters())
        }
        debug_info['ai_config'] = ai_config
        
        # 5. Ê£ÄÊü•ÁºìÂ≠òÁä∂ÊÄÅ
        cache_data = get_cache_record(db, compound_id, template_id)
        debug_info['cache_exists'] = cache_data is not None
        if cache_data:
            debug_info['cache_info'] = {
                'batch_count': len(cache_data.get('batchData', [])),
                'last_updated': cache_data.get('lastUpdated'),
                'file_hashes_count': len(cache_data.get('fileHashes', []))
            }
        
        return ApiResponse(
            success=True,
            data=debug_info
        )
        
    except Exception as e:
        return ApiResponse(
            success=False,
            error=f"Debug failed: {str(e)}"
        )

# ============ Â¢ûÂº∫ÁöÑ‰∏ªË¶ÅÂ§ÑÁêÜÁ´ØÁÇπ ============

async def download_pdfs_if_needed():
    """
    Â¶ÇÊûú uploads/pdfs ÁõÆÂΩï‰∏∫Á©∫Ôºå‰ªéÂâçÁ´Ø‰∏ãËΩΩ PDF Êñá‰ª∂
    ËøôÊòØ‰∏Ä‰∏™‰∏ÄÊ¨°ÊÄßÁöÑËß£ÂÜ≥ÊñπÊ°à
    """
    pdf_directory = getattr(settings, 'PDF_DIRECTORY', settings.UPLOAD_DIR + '/pdfs')
    
    # Ê£ÄÊü•ÁõÆÂΩïÊòØÂê¶Â≠òÂú®‰∏î‰∏∫Á©∫
    if not os.path.exists(pdf_directory):
        os.makedirs(pdf_directory, exist_ok=True)
    
    existing_pdfs = glob.glob(os.path.join(pdf_directory, "*.pdf"))
    
    # Â¶ÇÊûúÂ∑≤Êúâ PDF Êñá‰ª∂ÔºåË∑≥Ëøá‰∏ãËΩΩ
    if existing_pdfs:
        print(f"üìÑ ÁõÆÂΩï‰∏≠Â∑≤Êúâ {len(existing_pdfs)} ‰∏™ PDF Êñá‰ª∂ÔºåË∑≥Ëøá‰∏ãËΩΩ")
        return
    
    print("üì• Ê£ÄÊµãÂà∞ PDF ÁõÆÂΩï‰∏∫Á©∫ÔºåÂºÄÂßã‰ªéÂâçÁ´Ø‰∏ãËΩΩÊñá‰ª∂...")
    
    # ÂâçÁ´Ø PDF Êñá‰ª∂ÁöÑ URL ÂàóË°®ÔºàÈúÄË¶ÅÊ†πÊçÆ‰Ω†ÁöÑÂÆûÈôÖÂâçÁ´ØÂú∞ÂùÄ‰øÆÊîπÔºâ
    frontend_base_url = "https://beone-d.beigenecorp.net/aimta/assets/pdfs"  # ‰øÆÊîπ‰∏∫‰Ω†ÁöÑÂâçÁ´ØÂú∞ÂùÄ
    
    pdf_files = [
        "BGNE_GQA_BGB-16673_DS_CR-C200727003-FPF24001_COA-US.pdf",
        "BGNE_GQA_BGB-16673_DS_CR-C200727003-FPF24002_COA-US.pdf", 
        "BGNE_GQA_BGB-16673_DS_CR-C200727003-FPF24003_COA-US.pdf",
        "BGNE_GQA_BGB-16673_DS_CR-C200727003-FPF24004_COA-US.pdf",
        "BGNE_ESQ_BGB-16673_DS_CM-C200727003-FPF25101_COA-US.pdf"
    ]
    
    downloaded_count = 0
    
    for pdf_filename in pdf_files:
        try:
            pdf_url = f"{frontend_base_url}/{pdf_filename}"
            local_path = os.path.join(pdf_directory, pdf_filename)
            
            print(f"üì• ‰∏ãËΩΩ: {pdf_filename}")
            
            response = requests.get(pdf_url, timeout=30)
            response.raise_for_status()
            
            # È™åËØÅÊòØÂê¶ÊòØÊúâÊïàÁöÑ PDF Êñá‰ª∂
            if not response.content.startswith(b'%PDF'):
                print(f"‚ö†Ô∏è  {pdf_filename} ‰∏çÊòØÊúâÊïàÁöÑ PDF Êñá‰ª∂ÔºåË∑≥Ëøá")
                continue
            
            # ‰øùÂ≠òÊñá‰ª∂
            with open(local_path, 'wb') as f:
                f.write(response.content)
            
            file_size = len(response.content)
            print(f"‚úÖ {pdf_filename} ‰∏ãËΩΩÊàêÂäü ({file_size} bytes)")
            downloaded_count += 1
            
        except requests.RequestException as e:
            print(f"‚ùå ‰∏ãËΩΩ {pdf_filename} Â§±Ë¥•: {e}")
        except Exception as e:
            print(f"‚ùå ‰øùÂ≠ò {pdf_filename} Â§±Ë¥•: {e}")
    
    print(f"üìä ‰∏ãËΩΩÂÆåÊàê: {downloaded_count}/{len(pdf_files)} ‰∏™Êñá‰ª∂")

@router.post("/process-directory", response_model=ApiResponse)
async def process_directory(
    request: DirectoryProcessRequest,  # ÊâÄÊúâÂèÇÊï∞ÈÉΩÈÄöËøá JSON body ‰º†ÈÄí
    db: Session = Depends(get_db)
):
    """Process all PDF files in the specified directory and extract batch analysis data"""
    try:
        await download_pdfs_if_needed()
        # ‰ªé request ÂØπË±°Ëé∑ÂèñÂèÇÊï∞
        force_reprocess = getattr(request, 'force_reprocess', False)
        
        print(f"Êî∂Âà∞ËØ∑Ê±Ç:")
        print(f"  - compound_id: {request.compound_id}")
        print(f"  - template_id: {request.template_id}")
        print(f"  - force_reprocess: {force_reprocess}")
        
        # ÂàùÂßãÂåñÁºìÂ≠òÁÆ°ÁêÜÂô®
        cache_manager = BatchDataCache(db)
        
        # ‰ΩøÁî®ÈÖçÁΩÆ‰∏≠ÁöÑPDFÁõÆÂΩï
        pdf_directory = getattr(settings, 'PDF_DIRECTORY', settings.UPLOAD_DIR)
        os.makedirs(pdf_directory, exist_ok=True)

        # Â¶ÇÊûúÁõÆÂΩï‰∏∫Á©∫ÔºåÊèê‰æõ‰∏ä‰º†ÊèêÁ§∫
        if not os.path.exists(pdf_directory):
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"PDF directory not found: {pdf_directory}. Please upload PDF files to this directory."
            )
        
        pdf_files = glob.glob(os.path.join(pdf_directory, "*.pdf"))
        
        if not pdf_files:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,  
                detail=f"No PDF files found in directory: {pdf_directory}. Please upload PDF files first."
            )
        
        # Â¶ÇÊûú‰∏çÊòØÂº∫Âà∂ÈáçÊñ∞Â§ÑÁêÜÔºåÊ£ÄÊü•ÁºìÂ≠ò
        if not force_reprocess:
            print(f"\nüîç Checking cache for compound: {request.compound_id}, template: {request.template_id}")
            cache_data = get_cache_record(db, request.compound_id, request.template_id)
            
            if cache_data:
                # Ê£ÄÊü•Êñá‰ª∂ÊòØÂê¶ÊúâÂèòÊõ¥
                current_hashes = calculate_file_hashes(pdf_directory)
                cached_hashes = cache_data.get("fileHashes", [])
                
                if set(current_hashes) == set(cached_hashes):
                    # Êñá‰ª∂Ê≤°ÊúâÂèòÊõ¥ÔºåËøîÂõûÁºìÂ≠òÊï∞ÊçÆ
                    print(f"‚úÖ Cache hit! Loading {len(cache_data['batchData'])} batches from cache")
                    print(f"üìÖ Cache last updated: {cache_data['lastUpdated']}")
                    
                    return ApiResponse(
                        success=True,
                        data={
                            "processedFiles": cache_data.get("processedFiles", []),
                            "failedFiles": [],
                            "totalFiles": cache_data.get("totalFiles", 0),
                            "batchData": cache_data["batchData"],
                            "status": "success",
                            "fromCache": True,
                            "message": f"Loaded {len(cache_data['batchData'])} batches from cache (last updated: {cache_data['lastUpdated']})"
                        }
                    )
                else:
                    print(f"üìù Files have changed since last cache, proceeding with processing...")
            else:
                print(f"‚ùå No cache found, proceeding with processing...")
        else:
            print(f"üîÑ Force reprocess requested, skipping cache check...")
        
        # ÊâßË°åÂéüÊúâÁöÑÊâπÈáèPDFÂ§ÑÁêÜÈÄªËæë
        print(f"\n{'='*80}")
        print(f"üöÄ COA BATCH ANALYSIS PROCESSING STARTED")
        print(f"{'='*80}")
        print(f"üìÅ Directory: {pdf_directory}")
        print(f"üìÑ Found {len(pdf_files)} PDF files")
        print(f"üß¨ Compound ID: {request.compound_id}")
        print(f"üìã Template ID: {request.template_id}")
        
        # ÊòæÁ§∫AIÊúçÂä°Áä∂ÊÄÅ
        if hasattr(settings, 'USE_AZURE_OPENAI') and settings.USE_AZURE_OPENAI:
            print(f"üîµ AI Service: Azure OpenAI ({getattr(settings, 'AZURE_OPENAI_DEPLOYMENT_NAME', 'Unknown')})")
        elif hasattr(settings, 'OPENAI_API_KEY') and settings.OPENAI_API_KEY:
            print(f"üü¢ AI Service: Standard OpenAI")
        else:
            print(f"‚ö†Ô∏è  AI Service: Not available")
        
        print(f"üß™ Test Parameters: {len(ai_extractor.get_test_parameters())} items")
        print(f"{'='*80}")
        
        batch_data_list = []
        processed_files = []
        failed_files = []
        
        for i, pdf_file in enumerate(pdf_files, 1):
            # ‰∏∫ÊØè‰∏™Êñá‰ª∂ÂàõÂª∫Áã¨Á´ãÁöÑ‰∫ãÂä°
            document = None
            
            # ‰ΩøÁî®Â≠ê‰∫ãÂä°ÊàñÊñ∞‰ºöËØù
            try:
                # ÂºÄÂßãÊñ∞‰∫ãÂä°
                with db.begin_nested():  # ‰ΩøÁî®ÂµåÂ•ó‰∫ãÂä°
                    filename = os.path.basename(pdf_file)
                    print(f"\nüìÑ Processing file {i}/{len(pdf_files)}: {filename}")
                    print("-" * 80)
                    
                    # Ê£ÄÊü•Êñá‰ª∂ÊòØÂê¶Â∑≤Â≠òÂú®
                    existing_doc = db.query(COADocument).filter(
                        COADocument.filename == filename,
                        COADocument.compound_id == UUID(request.compound_id)
                    ).first()
                    
                    if existing_doc:
                        print(f"‚ö†Ô∏è  Document already exists: {filename}, skipping...")
                        continue
                    
                    # ÂàõÂª∫Êï∞ÊçÆÂ∫ìËÆ∞ÂΩï - ‰øÆÂ§çÔºö‰ΩøÁî® ProcessingStatus.PROCESSING.value
                    document = COADocument(
                        compound_id=UUID(request.compound_id),
                        filename=filename,
                        file_path=pdf_file,
                        file_size=f"{os.path.getsize(pdf_file) / 1024:.2f} KB",
                        processing_status=ProcessingStatus.PROCESSING.value  # ‰øÆÂ§çÔºö‰ΩøÁî®Êûö‰∏æÂÄºËÄå‰∏çÊòØÁ°¨ÁºñÁ†ÅÂ≠óÁ¨¶‰∏≤
                    )
                    
                    db.add(document)
                    db.flush()  # Ëé∑ÂèñID‰ΩÜ‰∏çÊèê‰∫§
                    
                    # ÊèêÂèñPDFÊñáÊú¨
                    print("üìñ Extracting text from PDF...")
                    pdf_text = await pdf_processor.extract_text(pdf_file)
                    
                    if not pdf_text.strip():
                        raise Exception("No text content found in PDF")
                    
                    print(f"‚úÖ Extracted {len(pdf_text)} characters of text")
                    
                    # ‰ΩøÁî®AIÊèêÂèñÊâπÊ¨°Êï∞ÊçÆ
                    print("üîç Extracting COA batch analysis data...")
                    batch_data = await ai_extractor.extract_coa_batch_data(pdf_text, filename)
                    
                    # È™åËØÅÂíåÊ∏ÖÁêÜÊï∞ÊçÆ
                    batch_data = ai_extractor.validate_batch_data(batch_data)
                    
                    # Êõ¥Êñ∞ÊñáÊ°£Áä∂ÊÄÅ - ‰øÆÂ§çÔºö‰ΩøÁî® ProcessingStatus.COMPLETED.value
                    document.processing_status = ProcessingStatus.COMPLETED.value  # ‰øÆÂ§çÔºö‰ΩøÁî®Êûö‰∏æÂÄº
                    document.processed_at = datetime.utcnow()
                    
                    # ‰øùÂ≠òÊèêÂèñÁöÑÊâπÊ¨°Êï∞ÊçÆ
                    batch_number = batch_data.get('batch_number', '')
                    manufacture_date = batch_data.get('manufacture_date', '')
                    manufacturer = batch_data.get('manufacturer', '')
                    
                    # ‰øùÂ≠òÂü∫Êú¨ÊâπÊ¨°‰ø°ÊÅØ
                    basic_fields = [
                        ('batch_number', batch_number),
                        ('manufacture_date', manufacture_date),
                        ('manufacturer', manufacturer)
                    ]
                    
                    for field_name, field_value in basic_fields:
                        if field_value:
                            data = ExtractedData(
                                document_id=document.id,
                                field_name=field_name,
                                field_value=field_value,
                                confidence_score=0.95,
                                original_text=field_value
                            )
                            db.add(data)
                    
                    # ‰øùÂ≠òÊµãËØïÁªìÊûúÊï∞ÊçÆ
                    test_results = batch_data.get('test_results', {})
                    for test_param, result_value in test_results.items():
                        if result_value and result_value not in ['TBD', '']:
                            data = ExtractedData(
                                document_id=document.id,
                                field_name=test_param,
                                field_value=result_value,
                                confidence_score=0.90,
                                original_text=result_value
                            )
                            db.add(data)
                
                # Êèê‰∫§ÂµåÂ•ó‰∫ãÂä°
                db.commit()
                
                # Ê∑ªÂä†Âà∞ÊàêÂäüÂàóË°®
                batch_data_list.append(batch_data)
                processed_files.append(filename)
                
                # ÊòæÁ§∫Â§ÑÁêÜÊëòË¶Å
                print(f"\n‚úÖ Successfully processed: {filename}")
                print(f"üì¶ Batch: {batch_number}")
                print(f"üìÖ Date: {manufacture_date}")
                print(f"üß™ Test Results: {len([v for v in test_results.values() if v not in ['TBD', 'ND', '']])}/{len(test_results)}")
                
            except Exception as e:
                # ÂõûÊªöÂµåÂ•ó‰∫ãÂä°
                db.rollback()
                
                error_msg = str(e)
                filename = os.path.basename(pdf_file) if 'pdf_file' in locals() else "unknown"
                print(f"‚ùå Error processing {filename}: {error_msg}")
                failed_files.append({"filename": filename, "error": error_msg})
                
                # Â¶ÇÊûúÊñáÊ°£Â∑≤ÂàõÂª∫ÔºåÂ∞ùËØïÊõ¥Êñ∞Áä∂ÊÄÅ‰∏∫Â§±Ë¥•
                if document and document.id:
                    try:
                        # ‰ΩøÁî®Êñ∞ÁöÑÂµåÂ•ó‰∫ãÂä°Êù•Êõ¥Êñ∞Â§±Ë¥•Áä∂ÊÄÅ
                        with db.begin_nested():
                            fail_doc = db.query(COADocument).filter(
                                COADocument.id == document.id
                            ).first()
                            if fail_doc:
                                # ‰øÆÂ§çÔºö‰ΩøÁî® ProcessingStatus.FAILED.value
                                fail_doc.processing_status = ProcessingStatus.FAILED.value  # ‰øÆÂ§çÔºö‰ΩøÁî®Êûö‰∏æÂÄº
                                fail_doc.error_message = error_msg[:500]  # ÈôêÂà∂ÈîôËØØÊ∂àÊÅØÈïøÂ∫¶
                        db.commit()
                    except Exception as update_error:
                        print(f"Failed to update document status: {update_error}")
                        db.rollback()
                
                continue
        
        # Â¶ÇÊûúÂ§ÑÁêÜÊàêÂäüÔºåÊõ¥Êñ∞ÁºìÂ≠ò
        if batch_data_list:
            try:
                print(f"\nüíæ Updating cache...")
                current_hashes = calculate_file_hashes(pdf_directory)
                cache_updated = update_cache_record(
                    db, request.compound_id, request.template_id, 
                    batch_data_list, current_hashes, processed_files
                )
                if cache_updated:
                    print(f"‚úÖ Cache updated successfully")
                else:
                    print(f"‚ö†Ô∏è Cache update failed")
            except Exception as e:
                print(f"‚ö†Ô∏è Cache update error: {e}")
        
        # Â§ÑÁêÜÂÆåÊàêÂêéÁöÑÊ±áÊÄª
        print(f"\n{'='*80}")
        print(f"üìà BATCH ANALYSIS PROCESSING SUMMARY")
        print(f"{'='*80}")
        print(f"üìÑ Total files found: {len(pdf_files)}")
        print(f"‚úÖ Successfully processed: {len(processed_files)}")
        print(f"‚ùå Failed: {len(failed_files)}")
        print(f"üìä Total batches analyzed: {len(batch_data_list)}")
        
        if failed_files:
            print(f"\n‚ùå Failed files:")
            for failed in failed_files:
                print(f"   ‚Ä¢ {failed['filename']}: {failed['error']}")
        
        if batch_data_list:
            print(f"\n‚úÖ Successfully processed batches:")
            for batch_data in batch_data_list:
                batch_num = batch_data.get('batch_number', 'Unknown')
                mfg_date = batch_data.get('manufacture_date', 'Unknown')
                test_count = len([v for v in batch_data.get('test_results', {}).values() if v not in ['TBD', 'ND', '']])
                print(f"   ‚Ä¢ {batch_num} (Mfg: {mfg_date}) - {test_count} test results")
        
        print(f"\nüîÑ Data Structure:")
        print(f"   ‚Ä¢ Each batch maintains independent data (no merging)")
        print(f"   ‚Ä¢ Ready for table generation with {len(batch_data_list)} columns")
        print(f"   ‚Ä¢ Test parameters: {len(ai_extractor.get_test_parameters())} items")
        print(f"{'='*80}")
        
        # ÂáÜÂ§áËøîÂõûÊï∞ÊçÆÔºà‰øùÊåÅÊØè‰∏™ÊâπÊ¨°Áã¨Á´ãÔºâ
        return ApiResponse(
            success=True,
            data={
                "processedFiles": processed_files,
                "failedFiles": failed_files,
                "totalFiles": len(pdf_files),
                "batchData": batch_data_list,  # ÊâπÊ¨°Êï∞ÊçÆÂàóË°®Ôºå‰∏çÂêàÂπ∂
                "status": "success" if processed_files else "failed",
                "fromCache": False,
                "cacheUpdated": len(batch_data_list) > 0,
                "message": f"Successfully processed {len(batch_data_list)} batches for COA analysis"
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        error_msg = str(e)
        print(f"\n‚ùå Directory processing failed: {error_msg}")
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=error_msg
        )

# ============ ‰øùÁïôÂéüÊúâÁöÑÂÖ∂‰ªñÁ´ØÁÇπ ============

@router.get("/batch-analysis/{compound_id}", response_model=ApiResponse)
async def get_batch_analysis_data(
    compound_id: UUID,
    db: Session = Depends(get_db)
):
    """Get all batch analysis data for a compound"""
    try:
        # Ëé∑ÂèñËØ•ÂåñÂêàÁâ©ÁöÑÊâÄÊúâÊñáÊ°£ - ‰øÆÂ§çÔºö‰ΩøÁî® ProcessingStatus.COMPLETED.value
        documents = db.query(COADocument).filter(
            COADocument.compound_id == compound_id,
            COADocument.processing_status == ProcessingStatus.COMPLETED.value  # ‰øÆÂ§çÔºö‰ΩøÁî®Êûö‰∏æÂÄº
        ).all()
        
        if not documents:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"No processed documents found for compound {compound_id}"
            )
        
        batch_data_list = []
        
        for document in documents:
            # Ëé∑ÂèñËØ•ÊñáÊ°£ÁöÑÊâÄÊúâÊèêÂèñÊï∞ÊçÆ
            extracted_data = db.query(ExtractedData).filter(
                ExtractedData.document_id == document.id
            ).all()
            
            if extracted_data:
                # ÈáçÊûÑÊâπÊ¨°Êï∞ÊçÆ
                batch_data = {
                    "filename": document.filename,
                    "batch_number": "",
                    "manufacture_date": "",
                    "manufacturer": "",
                    "test_results": {}
                }
                
                for data in extracted_data:
                    if data.field_name == "batch_number":
                        batch_data["batch_number"] = data.field_value
                    elif data.field_name == "manufacture_date":
                        batch_data["manufacture_date"] = data.field_value
                    elif data.field_name == "manufacturer":
                        batch_data["manufacturer"] = data.field_value
                    else:
                        batch_data["test_results"][data.field_name] = data.field_value
                
                batch_data_list.append(batch_data)
        
        return ApiResponse(
            success=True,
            data={
                "compoundId": str(compound_id),
                "totalBatches": len(batch_data_list),
                "batchData": batch_data_list
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )

@router.post("/upload", response_model=ApiResponse)
async def upload_document(
    file: UploadFile = File(...),
    compound_id: str = Form(...),
    template_id: Optional[str] = Form(None),
    db: Session = Depends(get_db)
):
    """Upload a COA document (legacy endpoint)"""
    try:
        if not file.filename.endswith('.pdf'):
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Only PDF files are allowed"
            )
        
        if file.size > settings.MAX_FILE_SIZE:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"File size exceeds maximum allowed size of {settings.MAX_FILE_SIZE / 1024 / 1024}MB"
            )
        
        file_path = await file_manager.save_upload(file, compound_id)
        
        # ‰øÆÂ§çÔºö‰ΩøÁî® ProcessingStatus.PENDING.value
        document = COADocument(
            compound_id=UUID(compound_id),
            filename=file.filename,
            file_path=file_path,
            file_size=f"{file.size / 1024:.2f} KB",
            processing_status=ProcessingStatus.PENDING.value  # ‰øÆÂ§çÔºö‰ΩøÁî®Êûö‰∏æÂÄºËÄå‰∏çÊòØÁ°¨ÁºñÁ†ÅÂ≠óÁ¨¶‰∏≤
        )
        
        db.add(document)
        db.commit()
        db.refresh(document)
        
        return ApiResponse(
            success=True,
            data={
                "documentId": str(document.id),
                "filename": document.filename,
                "status": document.processing_status
            }
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )